# End-to-end GAN-TTS architecture
A tensorflow implementation of GAN-TTS paper.

![Proposed architecture](https://github.com/alexandruRopotica/E2E-GANTTS/blob/main/Images/gantts.png)
### Notes
- Text embeddings are generated by a tensorflow pre-trained BERT model.
- Linguistic features are not predicted by external models, but they are predicted by a feature net that works together with the generator and the discriminator. The feature net is a simple CBHG module, which takes a text embedding in input and outputs a tensor of linguistic features.
- You can explore the data flow and data dimensionality using the notebook [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/alexandruRopotica/E2E-GANTTS/blob/main/E2EGANTTS.ipynb). The discriminator used in the notebook is different because colab GPU couldn't handle the original discriminator
- I trained the model on a really small dataset, 17 audio-texts from LJSpeech, because i didn't have a proper machine to use.
- To evaluate this GAN i used the Frech√©t Distance, where all embeddings were calcuated with VGGish TensorFlow pre-trained model.
### Papers
- [GAN-TTS](https://arxiv.org/abs/1909.11646) - Used to implement generator and discriminator
- [Tacotron](https://arxiv.org/abs/1703.10135) - Used to implement CBHG module
### External code
- [Spectral normalization](https://github.com/thisisiron/spectral_normalization-tf2)
- [Pre-trained BERT](https://huggingface.co/)
- [VGGish](https://tfhub.dev/google/vggish/1)
